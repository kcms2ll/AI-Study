# 【ラビットチャレンジ】　深層学習day1

以下は、JDLA E資格の認定プログラム「ラビットチャレンジ」における深層学習day1のレポートである。  
深層学習モデルについてレポートにまとめた。
* ニューラルネットワーク
* 学習

***
## 深層学習について
* 識別と生成
深層学習には、機械（深層）学習モデルを入力・出力の目的で分類したもの  

|   | 識別　| 生成 |
| --- | --- | ---   |
| 目的　| データを暮らすに分類<br>eg.犬や猫の画像の識別 | 特定のクラスのデータを生成<br>eg.猫らしい画像を生成 |
| 計算結果 | P(c_k\|k)<br>条件「あるデータxが与えられた」というもとで、クラスc_kとなる確率 | P(k\|c_k)<br>条件「あるクラスxに属するという」というもとで、データc_kとなる分布|
| モデル | 決定木、ロジステック回帰、SVM、ニューラルネットワークなど | 隠れマルコフモデル、ベイジアンネットワーク、変分オートエンコーダ（VEA）、敵対的生成ネットワーク（GAN）など |
| 特徴 | 高次元→低次元<br>必要な学習データ→多い<br>eg.画像認識 | 低次元→高次元<br>必要な学習データ→少ない<br>eg.画像の超解像、テキスト生成など |

* 識別機の開発アプローチ

|   | 識別モデル　| 識別モデル | 識別関数 |
| --- | --- | ---   | --- |
| 特徴 | データを人工的に生成<br>確率的な識別 | 確率的な識別 | 学習量が少ない<br>決定的な識別 |
| 学習コスト | 大 | 中 | 小 |  

## ニューラルネットワーク

一般的に中間層が2層以上(全体は、入出力層の+2層で4層以上)あるネットワークのことを深層ニューラルネットワークという。ニューラルネットワークは、回帰と分類ともに解くことができる。機械学習の手法の1つである。

1. 入力層〜中間層  
入力、重みは行列で表すことができ、総入力：u=Wx+bの形で表すことができる。y=axだと原点を通る直線しか表せないが、+bすることにより、水平移動ができるようになるので、yが実数全体を表現できるようになる。

* 確認テスト  
Q. ディープラーニングは何をしようとしているのか。  
A. 明示的なプログラムの代わりに多数の中間層を持つニューラルネットワークを用いて、入力値から目的とする出力値に変換する数学モデルを構築すること。  
Q.　最適化の最終目標  
A. 重みとバイアス  
Q. 次のネットワークを紙にかけ。  
A. 紙に記述のため省略  
Q. この図式に動物分対の実例を入れてみよう。  
A. 体長、体重、ひげの本数、毛の平均長、耳の大きさ、眉間・目鼻の距離、足の長さ  
Q. u=Wx+bをpythonのコードでかけ。  
A. 
    ```python
    W = [w_1,...,w_l]、x=[x1,...,xl]とすると
    u = np.(x, W) + b
    ```
    Q. 1−1のファイルから中間層の出力を定義しているソースを抜き出せ。  
    A. z=functions.rule(u)

2. 中間層  
* 活性化関数  
次の層への出力の大きさを決める非線形の関数。ニューラルネットワークでできる処理の幅が大きく広がる。

    1. ステップ関数  
    現在は使われていない。oか1しか表現できず、線形分離可能なものしか学習できない。

    1. シグモイド関数  
    0~1を緩やかに変化する関数。値の強弱を伝えられるようになった。大きな値の出力の感化が微小なため、勾配消失問題を引き起こす可能性がある。

    1. ReLU関数  
    現在最も利用されている関数。勾配消失問題の回避とスパース化に貢献することで、よい結果をもらす。

* 確認テスト  
Q. 線形と非線形の違いを図に書いて簡易に説明せよ。  
A. 線形関数は加法正で、斉次性を満たす。  
<br>
Q. 配布されたソースコードより該当する箇所を抜き出せ。  
A. z = functions.sigmoid(u)

3. 出力層  
中間層から得た数値を目的のものに変換する。誤差関数により学習を行う。誤差関数をが二乗誤差の場合、以下のように誤差を表せる。  
<img src="https://latex.codecogs.com/png.latex?\large&space;{\color{White}&space;E_{n}(W)&space;=&space;\frac{1}{2}\left&space;\|&space;y-d&space;\right&space;\|^{2}}" title="\large {\color{White} E_{n}(W) = \frac{1}{2}\left \| y-d \right \|^{2}}" />  
出力層は、どんな問題を得かによってその活性化関数が一つに定まる。

|   | 回帰　| 二値分類| 多クラス分類 |
| --- | --- | ---  | ---  |
| 活性化関数　| 恒等関数<br>※何もしない関数 | シグモイド関数 | ソフトマックス関数
| 誤差関数 | 二乗誤差 | 交差エントロピー | 交差エントロピー |

* 確認テスト  
Q. なぜ引き算ではなく二乗するか。また下式の1/2はどういう意味を持つか述べよ。
引き算で計算を行うと、誤差が必ず0となってしまい誤差関数が作れない。また二乗することで、誤差が性の値になる。1/2する理由は、誤差関数を微分するが、その際に二乗係数2がでてくるのと打ち消すためで、本質的な意味はない。  
<br>
Q. ①~③の数式に該当するソースコードを示し、一行づつ処理の説明をせよ。  
①→def softmax(s):  
②→np.exp(x)  
③→np.sum(np.exp(x))  
1行目：関数定義、2行目：次元数判定、3行目：行列の掛け算をするため転置、4行目オーバーフロー対策、5行目：ソフトマックスの演算、6行目return、7行目：オーバーフロー対策、8行目：ソフトマックスの演算。  
<br>
①→defcross_entropy_error(d, y):  
②→-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size  
1行目：関数定義、2行目：次元数判定、3行目：1行に展開、4行目：1行に展開、5行目：one-hot-vectorか判定、6行目：最大平均値を取得、7行目：バッチサイズを取得、8行目：クロスエントロピーを演算。

## ニューラルネットワークの学習について
ニューラルネットワークは、学習を通して誤差を最小にするネットワークを作成する。つまり、誤差E(w)を最小にするパラメータwを発見することがゴールとなる。そのためのパラメータの最適化の方法を勾配降下法という。  
<img src="https://latex.codecogs.com/png.latex?\large&space;{\color{White}&space;W^{(t&plus;1)}&space;=&space;W^{t}-\varepsilon&space;\nabla&space;E}" title="\large {\color{White} W^{(t+1)} = W^{t}-\varepsilon \nabla E}" />（εは学習率）→式1とする  
このとき、εが小さすぎても大きすぎても学習がうまく進まないことがある。

* 勾配降下法の種類
1. 確率的勾配降下法  
ランダムに抽出したサンプルの誤差を使って学習する。メリットが3つある。  
データが冗長な場合、計算コストが軽減される。また望まない局所最適解に収束するリスクも（毎回利用するデータセットが異なるため）軽減される。オンライン学習ができる。

1. ミニバッチ勾配降下法  
ミニバッチに分割したデータの集合D_tに属するサンプルの平均誤差を使って学習する。確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる。

* 確認テスト  
Q. 式1の該当するソースコードを探してみよう。  
A. network[key]  -= learning_rate* grad[key]  
grad = backward(x, d, z1, y)  
Q. オンライン学習とは何か2行でまとめよ。  
A. 学習データが入ってくるたびに都度パラメータを更新し、学習を勧めていく方法。  
Q. この数式（式1）の意味を図に書いて説明せよ。  
A.　紙に書いたため省略。  

* 確認テスト  
Q. 誤差逆伝播法では不要な再帰的処理を避ける事が出来る。既に行った計算結果を保持しているソースコードを抽出せよ。  
Q. 2つの空欄に該当するソースコードを探せ。  
A. delta2 = functions.d_mean_squared_error(d, y)  
A. grad['W2'] = np.dot(z1.T, delta2)  

* [ニューラルネットワークの実装](https://github.com/kcms2ll/AI-Study/blob/main/ETest/src/neural_network.ipynb)  
※順伝播、活性化関数、出力層、勾配降下法、誤差逆伝播法について