# 【ラビットチャレンジ】　深層学習day1

以下は、JDLA E資格の認定プログラム「ラビットチャレンジ」における深層学習day2のレポートである。  
深層学習モデルについてレポートにまとめた。
* 深層学習を行う上での学習テクニック
    * 勾配消失問題
    * 学習手法最適化
    * 過学習
* 畳み込みニューラルネットワーク

***
## 深層学習を行う上での学習テクニック
## 1.勾配消失問題
深層学習において、中間層を増やすと色々な表現ができるようになるが、同時にそれまでの学習の仕方だとうまくいかないことも出てくる。  
それが、勾配消失問題である。勾配消失問題とは、誤差伝播法が下位層に進んでいくにつれて、勾配がどんどん緩やかになっていき、勾配降下法による更新では、下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなる問題である。

* 確認テスト  
Q. 連鎖律の原理を使い、dz/dxを求めよ。  
<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;\left\{\begin{matrix}Z=t^{2}&space;\\&space;t=x&plus;y&space;\end{matrix}\right.}" title="{\color{White} \left\{\begin{matrix}Z=t^{2} \\ t=x+y \end{matrix}\right.}" />  
A.  
<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;\frac{dz}{dx}=\frac{dz}{dt}\frac{dt}{dx}=2t\times&space;1=2(x&plus;y)}" title="{\color{White} \frac{dz}{dx}=\frac{dz}{dt}\frac{dt}{dx}=2t\times 1=2(x+y)}" />  
<br>
Q. シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。
(1)0.15 ,(2)0.25,(3)0.35,(4)0.45  
A. 2番

* 解決法1:活性化関数  
活性化関数であるシグモイド関数により引き起こされるので、ReLU関数を利用する。
逆伝搬において、x>0のときは、1で、x<=0では、0となるため、値が小さくならず、また、0のときはその部分の重みが消えるスパース化にも貢献する。

* 解決方2:初期値の設定方法  
2.1. Xavier  
Xavierの初期値を設定する際の活性化関数は、ReLU、シグモイド、双極線正接関数に対し利用できる。重みの要素を前の層のノード数の平方根で除算する。  
<br>
2.2 He
Heの初期値を設定する際の活性化関数は、ReLU。重みの要素をを前の層のノード数の平方根で除算した値に対し、√2をかけ合わせた値。

* 確認テスト  
Q. 重みの初期値に0を設定すると、どのような問題が発生するか。簡潔に説明せよ。
A. 正しい学習が行えない問題が発生する。全ての重みの値が均一に更新されるため、多数の重みを持っている意味がなくなる。

* 解決法3:バッチ正規化  
ミニバッチ単位で入力のデータの偏りを抑制する方法。活性化関数に値を渡す前後にバッチ正則化の処理を含んだ層を加える。これによって、中間層の重みの更新が安定し、早く学習が進み、時間短縮される。また、ミニバッチ単位で正規化を行うので、学習データのばらつきを抑制し、過学習を防ぐ。  

* 確認テスト  
Q. 一般的に考えられるバッチ正規化の効果を2点挙げよ。  
A. ①中間層の重みの更新が安定し、早く学習が進み、時間短縮される。  
A. ②ミニバッチ単位で正則化を行うので、学習データのばらつきを抑制し、過学習を防ぐ。  

## 2.学習率最適化手法  
深層学習の最終目標は、誤差を最小にするネットワークを作成することである。つまり、誤差E(w)を最小化するパラメータwを探すことである。そして、そのwを見つける方法に勾配降下法を用いたパラメータの最適化が挙げられる。  
この勾配降下法には学習率というものがあるが、それが最適な値でないと、学習に時間がかかったり、最適解に辿り着けないということが起こる。これらを解消する方法が4つほど存在する。  
* 解決法1:モメンタム  
誤差をパラメータで微分したもとと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する。これにより、局所最適解にならず、大域最適解となる。谷間についてからの最も低い位置（最適解）にいくまでの時間が短いなどの利点がある。

* 解決法2:Adagrad  
誤差をパラメータで微分したものと再定義した学習率の積を減算する。勾配の緩やかな斜面に対し、最適解に近づけるという利点がある。ただ、学習率が徐々に小さくなるので、鞍点問題を引き起こす可能性がある。  
※鞍点とは、最適化対象の関数が鞍のような形になっている部分。どの方面に対しても平坦で勾配がゼロに近くなる。

* 解決法3:RMSProp  
誤差をパラメータで微分したものと再定義した学習率の積を減算する。adagradにαを追加したもので、あるだが小さいと昔の勾配情報がある程度無視できる。  
これにより、局所最適解にはならず、大域的最適解となる。ハイパーパラメータの調整が必要な場合が少ないなどの利点がある。

* 解決法4:Adam  
モメンタムの過去の勾配の指数関数的減衰平均とRMSPropの過去の勾配の二乗の指数関数的減衰平均の2つを含んだアルゴリズムで、両方のいいとこどりをしたもの。

* 確認テスト  
Q. モメンタム・AdaGrad・RMSPropの特徴をそれぞれ簡潔に説明せよ。  
A. モメンタム  
局所的最適解にはならず、大域的最適解になる。谷間についてから最も低い位置（最適値）に行くまでの時間が短い。
<br>
A. AdaGrad  
勾配の緩やかな斜面に対して、最適値に近づける。
<br>
A. RMSProp  
局所的最適解にはならず、大域的最適解となる。ハイパーパラメータの調整が必要な場合が少ない。

## 3.過学習
モデルを作成し、訓練データは正しく認識できるが、テストデータに対し、うまく認識できないということを過学習という。発生要因としては、パラメータの数が多かったり、パラメータの値が適切でなかったり等がある。また、ネットワークの自由度が高い状態にあることも要因のひとつである。

* 解決法1:正則化  
過学習の発生要因として、重みが大きい場合がある。学習させていくと、重みにばらつきが発生する。重みが大きい値は学習に重要な値であり、重みが大きいとか学習につながる。それを解決するのが、正則化である。誤差に対して、正則化項を追加することで重みを解決する。  
※過学習が起こりそうな重みの大きさ以下で重みをコントロールし、かつ重みの大きさにばらつきを出す必要がある。  
    * L1、L2正則化  
    誤差関数にPノルム（距離）を加えることで、正則化をする。

* 解決法2:ドロップアウト  
ニューラルネットワークのノードが多いと過学習することがある。なので、ランダムにノードを削除し学習する。元のデータをノードを削除することにより、あたかもデータが増えたかのような状態を起こせる。

* 確認テスト  
Q. 下図について、L1正則化を表しているグラフはどちらか答えよ。
A. 右図

## 畳み込みニューラルネットワーク(CNN)
次元間で繋がりのあるデータを扱える。

* 畳み込み層
画像データの場合、畳み込み層では、縦、横、チャンネル（色合い）、の3次元データをそのまま学習し、次に伝えることができる。つまり、空間情報を学習できるような層となっている。
    * パディング  
    畳み込みの処理により、出力サイズが小さくなるのを防ぐ。ゼロで埋めることが多いが、ゼロ以外でも問題ない。
    * ストライド  
    畳み込みを適用する感覚を指す。
    * チャンネル  
    畳み込みをする際の利用するフィルターの数。

* プーリング層  
対象領域の最大値(Max Pooling)、または平均値(Average Pooling)を取得する。（情報を圧縮しているイメージ）

* 全結合層  
3次元だったデータを出力したい形（1,2次元の）のデータに変換し、出力する。flatten,Global Max Pooling,Global Average Poolingの3手法がある。

* 確認テスト  
Q. サイズ6×6の入力画像を、サイズ2×2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドとパディングは1とする。  
A. 7×7

***

## ステージテスト3の復習

* Q. 正規化線形カーネル<img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;k(x^{T},x)=\frac{x^{T}x^{'}}{||x||&space;\,&space;||x^{'}||}}" title="{\color{White} k(x^{T},x)=\frac{x^{T}x^{'}}{||x|| \, ||x^{'}||}}" />|に対応するA. 特徴ベクトルφ(x)はどう表せるか。  
<br>
(𝒙,𝒙′)=𝝓(𝒙)𝑻𝝓(𝒙)で定義され，  
<img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\frac{x^{T}x^{'}}{\left&space;\|&space;x&space;\right&space;\|\left&space;\|&space;x^{'}&space;\right&space;\|}=(\frac{x}{\left&space;\|&space;x&space;\right&space;\|}&space;)^{T}(\frac{x^{'}}{\left&space;\|&space;x&space;\right&space;\|})}" title="{\color{White} \frac{x^{T}x^{'}}{\left \| x \right \|\left \| x^{'} \right \|}=(\frac{x}{\left \| x \right \|} )^{T}(\frac{x^{'}}{\left \| x \right \|})}" />なので、<img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\phi&space;(x)=\frac{x}{\left&space;\|&space;x&space;\right&space;\|}}" title="{\color{White} \phi (x)=\frac{x}{\left \| x \right \|}}" />

* Q. 2次元空間において「(-1, -1), (0, 0), (1, 1)」という3つデータ点が与えられ，これらのデータに対して主成分分析を適用する。問題前の問題で１次元空間に射影されたデータを元の２次元空間に再構成すると、再構成誤差（元のデータと再構成したデータとの誤差）は何%か。  
<br>
A. データ点は一直線上にあるので、第1主成分に射影しても情報は損失しないので、再構成によって完全にもとの情報を復元できる。つまり再構成誤差は0%である。

