# 【ラビットチャレンジ】　機械学習（後半）

以下は、JDLA E資格の認定プログラム「ラビットチャレンジ」における機械学習のレポートである。  
線形回帰モデルについてレポートにまとめた。
* 分類モデル
    * ロジスティック回帰モデル
    * 主成分分析
    * KNN
    * k-means
* 評価手法

***
## 分類問題
ある入力（数値から）クラスに分類する問題

* 分類で扱うデータ  
    * 入力（各要素を説明変数または特徴量と呼ぶ）、m次元ベクトル  
    * 出力、0or1の値  


## ロジスティック回帰モデル
分類問題を解くための教師あり学習モデル（教師データから学習）。入力m次元パラメータの線形結合をシグモイド関数に入力する。結果はy=1となる”確率”で出力される。  
ベルヌーイ分布を利用している。ある分布を考えたとき、パラメータ（既知）によって生成されるデータは変化する。例えば、コインの表が出る確率が0.3や0.8の場合、出る結果（表が出る数）が変わる。  
また、データからそのデータを生成したであろう尤もらしい分布（パラメータ）を推定するとき、これを最尤推定という。今回モデルを推定する場合には、後者のデータからパタメータを推定するパターンで考えている。  
* シグモイド関数  
入力は実数。出力は必ず、[0,1]の値。  
    * シグモイド感薄の微分  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\sigma&space;=&space;\left&space;\{&space;1&plus;exp\left&space;(&space;-ax&space;\right&space;)&space;\right&space;\}^{-1}}" title="{\color{White} \sigma = \left \{ 1+exp\left ( -ax \right ) \right \}^{-1}}" />  
    より、  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\Leftrightarrow&space;\cdots&space;}" title="{\color{White} \Leftrightarrow \cdots }" />  （間の計算省略）  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\Leftrightarrow&space;a(z)\times&space;(1-a(z))}" title="{\color{White} \Leftrightarrow a(z)\times (1-a(z))}" />  
    となる。自分自身で表されるため、微分などを行う際は、計算コストが軽減される。
* 尤度関数  
尤度関数とは、データを固定し、パラメータを変化させる関数を指す。尤度関数を最大化するようなパラメータを選ぶ方法を最尤推定（最尤法）という。
    * 尤度関数の一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;P(y_{1},y_{2},\cdots&space;,y_{n};P)&space;=&space;\prod_{i=1}^{n}P^{y_{i}}(1-P)^{1-y_{i}}}" title="\large {\color{Red} P(y_{1},y_{2},\cdots ,y_{n};P) = \prod_{i=1}^{n}P^{y_{i}}(1-P)^{1-y_{i}}}" />  
    ロジスティック回帰では、y1...ynは既知で、pの値が最大となる場合を推定したい。0<1<pなので、尤度関数は掛け算されることによって、微小な値になるかもしれない。そこで、対数を取って回避する。これを対数尤度関数という。（対数は単調増加なので、尤度関数が最大となる点と、対数尤度関数が最大となる点は同じ。）  
    ただ、対数尤度関数を微分して0になる点を求める必要があるが、解析的にこの値を求めるのは困難である。ここで登場するのが、勾配降下法である。
* 勾配降下法  
反復学習によりハイパーパラメータを逐次的に更新するアプローチの１つである。ηを学習率と呼ばれるパラメータでモデルのパタメータの収束しやすさを調整する。勾配降下法において、パラメータが更新されなくなった場合、それは、勾配がなくなったということで、少なくとも反復学習で創作した範囲では、最適な解が求められたことになる。  
    * 一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;\boldsymbol{w}^{(k&plus;1)}&space;=&space;\boldsymbol{w}^{(k)}&space;&plus;&space;\eta&space;\sum_{i=1}^{n}(y_{i}-p_{i})\boldsymbol{x_{i}}}" title="\large {\color{White} \boldsymbol{w}^{(k+1)} = \boldsymbol{w}^{(k)} + \eta \sum_{i=1}^{n}(y_{i}-p_{i})\boldsymbol{x_{i}}}" />  
    この式から分かるように、勾配降下法はパタメータを更新するのに、N個すべてのデータに対する和を求める必要がある。それゆえ、mが巨大になったときにデータがメモリの容量を超える可能性や、計算時間が膨大になるという懸念がある。これを解決するのが、次の確率的勾配降下法である。
* 確率的勾配降下法  
データを１つずつランダムに（確率的に）選んだパタメータを更新する。勾配降下法でパラメータを1回更新するのと同じ計算料でパラメータをn回更新できるので、効率よく最適な解を探索可能となる。  
    * 一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;\boldsymbol{w}^{(k&plus;1)}&space;=&space;\boldsymbol{w}^{(k)}&space;&plus;&space;\eta&space;(y_{i}-p_{i})\boldsymbol{x_{i}}}" title="\large {\color{White} \boldsymbol{w}^{(k+1)} = \boldsymbol{w}^{(k)} + \eta (y_{i}-p_{i})\boldsymbol{x_{i}}}" />

## 主成分分析
多数料データの持つ構造をより少数個の指標に圧縮。  
また、少数変数を利用した分析や可視化が実現可能。  
情報量の大きさを分散と捉えることができ、係数ベクトルが変わると線形変換後のデータが変化することになる。よって、線形変換後の変数の分散が最大となる射影を探索する

## KNN
最近傍のデータをk個とってきて、それが尤も多く所属するクラスに識別する。  
kを大きくすると境界線はよりなめらかになる。

## k-means
教師なし学習でクラスタリング手法の1つ。  
与えられたデータをk個のクラスに分類する。  
* アルゴリズム
    1. 各クラスタ中心の初期値を設定  
    ※初期値が近いと、うまくクラスタリングできないため、注意
    1. 各データ点に対し、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる
    1. 各クラスたの平均ベクトル（中心）を計算する
    1. 収束するまで、2,3を繰り返す

## SVM
2クラス分類を行う手法の1つ。拡張して、回帰や教師なし学習にも応用されている。

* 決定関数と分類境界  
2クラス分類の問題では、特徴（入力）ベクトルxがとちらのクラスに属するか判定するために、決定関数と呼ばれるf(x)が使われる。そのf(x)のwとbの値を決定した関数を分類境界と呼ぶ。SVMでは、それぞれのクラスのデータが分類境界からなるべく離れるようにして「最適な」分類境界を決定する。分類境界を挟んで2つのクラスがどのくらい離れているかをマージンと呼ぶSVMの視点では、マージンが大きいほど良い分類境界となるため、なるべく大きなマージンを持つ分類境界を探すことになる。このような考え方のことを一般に、マージン最大化と呼ぶ。

    * ハードマージン  
    n個の訓練データを全て正しく分類できるwとbの組が存在するという仮定のもと、決定関数を算出したもの。マージン最大化を目指しつつも、仮定した条件を満たすようなwとbの値を算出すると求められる。この仮定を制約条件という。マージン最大化、すなわち「分類境界f(x) = 0と分類境界から最も近くにあるデータxiとの距離の最大化」に基づいて分類境界が決定されるため、このxiのことをサポートベクトルという。

    * ソフトマージン  
    ハードマージンでは訓練データを完璧に分類できる決定関数f(x)が存在するという仮定を置いた。しかし、現実の多くの問題にとってこの仮定は強すぎるので、この仮定をなくし、SV分類を分離可能でないデータに適用できるように拡張していく。それがソフトマージンと呼ばれる。制約条件を緩和し、多少の分類誤りは許すようにすることで実現を目指す。マージン内に入るデータや誤分類されたデータに対する誤差を表す変数をξとし、スラック変数(slack variable)と呼ぶ。f(x) = 1とf(x) =1の間の距離をマージンと解釈し、ハードマージン同様、このマージンを最大化しつつ、分類の誤差ξを最小化するように分類境界を決定することを目的とする。  
    ソフトマージンでは、正則化係数という、学習前に与えておく必要があるハイパーパラメータがあり、この値が大きいほどハードマージンの場合に近づき、∞の場合、ハードマージンに一致する。逆に、この値が小さいほど誤分類が許容されやすくなる。

* SVMにおける双対表現  
一般にこれらの最適化問題のことを、SV分類の主問題(primalproblem)と呼ぶ。この主問題を解けば分類境界を決定できるが、SV分類ではこの主問題と等価な双対問題(dual problem)の方を解くことが一般的となっている。つまり、双対問題が、SV分類の場合には主問題と等価になっている。  
これは、  
    * 主問題と比べて双対問題の方が変数を少なくできる1
    * 分類境界の非線形化を考える上で双対問題の形式(双対形式)の方が有利となる
  
    の2点があげられるため、双対問題を解くことが一般的になっている。

* カーネルカトリック法  
データの分布により線形分離ではうまく分離できないケースに対し、特徴ベクトルを非線形変換し、その空間で線形分類を行う手法のこと。単純な直線で分類できそうにないデータを高次元に拡張することで、線形分離可能になる場合がある。また、カーネル関数(内積なのでスカラー)を用いることで、特徴空間が高次元の場合でも、双対問題を解く計算コストを大幅に削減することが可能となる。

***

## 確認テストの復習

Q. 確率的勾配降下法には、モメンタムという慣性項を追加することで学習速度を早める手法がある。モメンタムなしでは、<img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\theta&space;\leftarrow&space;\theta&space;-\eta&space;\nabla_{\theta&space;}L_{i}(\theta&space;)}" title="{\color{White} \theta \leftarrow \theta -\eta \nabla_{\theta }L_{i}(\theta )}" />と表せるが、モメンタムありではどのようになるか。  
A. <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\theta&space;\leftarrow&space;\theta&space;-\eta&space;\nabla_{\theta&space;}L_{i}(\theta&space;)&plus;\alpha&space;(\theta&space;_{i}-\theta&space;_{i-1})}" title="{\color{White} \theta \leftarrow \theta -\eta \nabla_{\theta }L_{i}(\theta )+\alpha (\theta _{i}-\theta _{i-1})}" />  
※α(慣性の度合い)×変化量と表すことができる。

Q. 完全に分類することのできないデータでロジスティック回帰を分類させるには？  
A. 工事の特徴空間へ写像したり、新たな特徴量にしてから適用する