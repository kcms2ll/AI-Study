# 【ラビットチャレンジ】　機械学習（後半）

以下は、JDLA E資格の認定プログラム「ラビットチャレンジ」における機械学習のレポートである。  
線形回帰モデルについてレポートにまとめた。
* 分類モデル
    * ロジスティック回帰モデル
    * 主成分分析
    * KNN
    * k-means
* 評価手法

***
## 分類問題
ある入力（数値から）クラスに分類する問題

* 分類で扱うデータ  
    * 入力（各要素を説明変数または特徴量と呼ぶ）、m次元ベクトル  
    * 出力、0or1の値  


## ロジスティック回帰モデル
分類問題を解くための教師あり学習モデル（教師データから学習）。入力m次元パラメータの線形結合をシグモイド関数に入力する。結果はy=1となる”確率”で出力される。  
ベルヌーイ分布を利用している。ある分布を考えたとき、パラメータ（既知）によって生成されるデータは変化する。例えば、コインの表が出る確率が0.3や0.8の場合、出る結果（表が出る数）が変わる。  
また、データからそのデータを生成したであろう尤もらしい分布（パラメータ）を推定するとき、これを最尤推定という。今回モデルを推定する場合には、後者のデータからパタメータを推定するパターンで考えている。  
* シグモイド関数  
入力は実数。出力は必ず、[0,1]の値。  
    * シグモイド感薄の微分  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\sigma&space;=&space;\left&space;\{&space;1&plus;exp\left&space;(&space;-ax&space;\right&space;)&space;\right&space;\}^{-1}}" title="{\color{White} \sigma = \left \{ 1+exp\left ( -ax \right ) \right \}^{-1}}" />  
    より、  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\Leftrightarrow&space;\cdots&space;}" title="{\color{White} \Leftrightarrow \cdots }" />  （間の計算省略）  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\Leftrightarrow&space;a(z)\times&space;(1-a(z))}" title="{\color{White} \Leftrightarrow a(z)\times (1-a(z))}" />  
    となる。自分自身で表されるため、微分などを行う際は、計算コストが軽減される。
* 尤度関数  
尤度関数とは、データを固定し、パラメータを変化させる関数を指す。尤度関数を最大化するようなパラメータを選ぶ方法を最尤推定（最尤法）という。
    * 尤度関数の一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;P(y_{1},y_{2},\cdots&space;,y_{n};P)&space;=&space;\prod_{i=1}^{n}P^{y_{i}}(1-P)^{1-y_{i}}}" title="\large {\color{Red} P(y_{1},y_{2},\cdots ,y_{n};P) = \prod_{i=1}^{n}P^{y_{i}}(1-P)^{1-y_{i}}}" />  
    ロジスティック回帰では、y1...ynは既知で、pの値が最大となる場合を推定したい。0<1<pなので、尤度関数は掛け算されることによって、微小な値になるかもしれない。そこで、対数を取って回避する。これを対数尤度関数という。（対数は単調増加なので、尤度関数が最大となる点と、対数尤度関数が最大となる点は同じ。）  
    ただ、対数尤度関数を微分して0になる点を求める必要があるが、解析的にこの値を求めるのは困難である。ここで登場するのが、勾配降下法である。
* 勾配降下法  
反復学習によりハイパーパラメータを逐次的に更新するアプローチの１つである。ηを学習率と呼ばれるパラメータでモデルのパタメータの収束しやすさを調整する。勾配降下法において、パラメータが更新されなくなった場合、それは、勾配がなくなったということで、少なくとも反復学習で創作した範囲では、最適な解が求められたことになる。  
    * 一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;\boldsymbol{w}^{(k&plus;1)}&space;=&space;\boldsymbol{w}^{(k)}&space;&plus;&space;\eta&space;\sum_{i=1}^{n}(y_{i}-p_{i})\boldsymbol{x_{i}}}" title="\large {\color{White} \boldsymbol{w}^{(k+1)} = \boldsymbol{w}^{(k)} + \eta \sum_{i=1}^{n}(y_{i}-p_{i})\boldsymbol{x_{i}}}" />  
    この式から分かるように、勾配降下法はパタメータを更新するのに、N個すべてのデータに対する和を求める必要がある。それゆえ、mが巨大になったときにデータがメモリの容量を超える可能性や、計算時間が膨大になるという懸念がある。これを解決するのが、次の確率的勾配降下法である。
* 確率的勾配降下法  
データを１つずつランダムに（確率的に）選んだパタメータを更新する。勾配降下法でパラメータを1回更新するのと同じ計算料でパラメータをn回更新できるので、効率よく最適な解を探索可能となる。  
    * 一般化  
    <img src="https://latex.codecogs.com/png.latex?\inline&space;\large&space;{\color{White}&space;\boldsymbol{w}^{(k&plus;1)}&space;=&space;\boldsymbol{w}^{(k)}&space;&plus;&space;\eta&space;(y_{i}-p_{i})\boldsymbol{x_{i}}}" title="\large {\color{White} \boldsymbol{w}^{(k+1)} = \boldsymbol{w}^{(k)} + \eta (y_{i}-p_{i})\boldsymbol{x_{i}}}" />

## 主成分分析
多数料データの持つ構造をより少数個の指標に圧縮。  
また、少数変数を利用した分析や可視化が実現可能。  
情報量の大きさを分散と捉えることができ、係数ベクトルが変わると線形変換後のデータが変化することになる。よって、線形変換後の変数の分散が最大となる射影を探索する

## KNN
最近傍のデータをk個とってきて、それが尤も多く所属するクラスに識別する。  
kを大きくすると境界線はよりなめらかになる。

## k-means
教師なし学習でクラスタリング手法の1つ。  
与えられたデータをk個のクラスに分類する。  
* アルゴリズム
    1. 各クラスタ中心の初期値を設定  
    ※初期値が近いと、うまくクラスタリングできないため、注意
    1. 各データ点に対し、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる
    1. 各クラスたの平均ベクトル（中心）を計算する
    1. 収束するまで、2,3を繰り返す

***

## 確認テストの復習

Q. 確率的勾配降下法には、モメンタムという慣性項を追加することで学習速度を早める手法がある。モメンタムなしでは、<img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\theta&space;\leftarrow&space;\theta&space;-\eta&space;\nabla_{\theta&space;}L_{i}(\theta&space;)}" title="{\color{White} \theta \leftarrow \theta -\eta \nabla_{\theta }L_{i}(\theta )}" />と表せるが、モメンタムありではどのようになるか。  
A. <img src="https://latex.codecogs.com/png.latex?\inline&space;{\color{White}&space;\theta&space;\leftarrow&space;\theta&space;-\eta&space;\nabla_{\theta&space;}L_{i}(\theta&space;)&plus;\alpha&space;(\theta&space;_{i}-\theta&space;_{i-1})}" title="{\color{White} \theta \leftarrow \theta -\eta \nabla_{\theta }L_{i}(\theta )+\alpha (\theta _{i}-\theta _{i-1})}" />  
※α(慣性の度合い)×変化量と表すことができる。

Q. 完全に分類することのできないデータでロジスティック回帰を分類させるには？  
A. 工事の特徴空間へ写像したり、新たな特徴量にしてから適用する