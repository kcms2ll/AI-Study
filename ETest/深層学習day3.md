# 【ラビットチャレンジ】　深層学習day3

以下は、JDLA E資格の認定プログラム「ラビットチャレンジ」における深層学習day3のレポートである。  
深層学習モデルについてレポートにまとめた。
* 再帰型ニューラルネットワーク
    * RNN
        * BPTT
    * LSTM
    * GRU
    * 双方向RNN
* RNNでの言語処理
    * Seq2Seq
    * Word2vec
    * AttentionMechanism

***
* 確認テスト  
Q. サイズ5×5の入力画像を、サイズ3×3のフィルタで畳み込んだときの出力画像のサイズを答えよ。なおスライドは2、パディングは1とする。  
A 3×3  

## 再帰型ニューラルネットワーク  
### RNN  
時系列モデルを扱うには、初期の状態と過去の時時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める構造が必要となる。

* 確認テスト
Q. RNNのネットワークには多く着分けて3つの重みがある。1つは入力から現在の中間層を定義する際にかけられる重み、1つは中間層から出力を定義する際にかけられる重みである。残り1つの重みについて説明せよ。  
A. 前の中間層から現在の中間層を定義する際にかけられる重み  

* BPTTとは？  
RNNにおいてパラメータ調整方法の1種（誤差逆伝播の1種）

* 確認テスト  
Q. 連鎖律の原理を使い、dz/dxを求めよ。  
<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;\left\{\begin{matrix}Z=t^{2}&space;\\&space;t=x&plus;y&space;\end{matrix}\right.}" title="{\color{White} \left\{\begin{matrix}Z=t^{2} \\ t=x+y \end{matrix}\right.}" />  
A.  
<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;\frac{dz}{dx}=\frac{dz}{dt}\frac{dt}{dx}=2t\times&space;1=2(x&plus;y)}" title="{\color{White} \frac{dz}{dx}=\frac{dz}{dt}\frac{dt}{dx}=2t\times 1=2(x+y)}" />  
Q. 下図のy1をx・s0・s1・win・w・woutを用いて数式で表せ。※バイアスは任意の文字で定義せよ。※また中間層の出力にシグモイド関数g(x)を作用させよ。  
A.  
<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;y_{1}=&space;g\left&space;(&space;W_{out}S_{1}&space;&plus;&space;c&space;\right&space;)}" title="{\color{White} y_{1}= g\left ( W_{out}S_{1} + c \right )}" />  
ただし、<img src="https://latex.codecogs.com/png.latex?{\color{White}&space;\left&space;(S_{1}&space;=&space;W_{in}x_{1}&space;&plus;&space;WS_{0}&space;&plus;&space;b&space;\right&space;)}" title="{\color{White} \left (S_{1} = W_{in}x_{1} + WS_{0} + b \right )}" />  

* [RNNの実装]()

### LSTM
RNNは時系列データを遡れば遡るほど、勾配が消失していく（勾配消失問題）。この問題を構造的に変えて、解決したものをLSTMという。  

* 確認テスト  
Q. シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。  
(1)0.15 ,(2)0.25,(3)0.35,(4)0.45  
A. (2)

* 勾配爆発  
勾配が、層を逆伝播するごとに指数関数的に大きくなっていくこと  

* CEC  
これまでの出力された情報を保持する機構。勾配消失および勾配爆発の解決方法として、勾配が、1であれば解決できる。

    * 過去情報を蓄積するだけで、学習ができないという課題  
    入力データについて、時間依存度に関係なく重みが一律である。つまり、ニューラルネットワークの学習特性がない状態となっている  
    ↓　これらの課題を解決するために…  
    * 入力ゲートと出力ゲート（LSTMブロック）の設置  
    それぞれのゲートへの入力値に対し、重みをかけることで、その時点の重みの重要度を調整できるようになった  
    ↓　LSTMブロックにもまだ課題がある…  
    * 過去の情報の制御ができないという課題  
    CECは、過去の情報が全て保存されている。過去の情報でも現在と関係ない情報が含まれていて、その情報を制御できていない。つまり、過去の情報が要らなくなった場合、削除することができず保管され続ける。  
    ↓これを解決するために…  
    * 忘却ゲートの設置  
    過去の情報がいらなくなったタイミングで情報を忘却する機能。

* 確認テスト  
Q. 以下の文章をLSTMに入力し空欄に当てはまる単語を予測したいとする。文中の「とても」という言葉は空欄の予測においてなくなっても影響を及ぼさないと考えられる。このような場合、どのゲートが作用すると考えられるか。「映画おもしろかったね。ところで、とてもお腹が空いたから何か____。  
A. 忘却ゲート  

* 覗き穴結合  
これまでの課題として、CECの保存されている過去の情報を、任意のタイミングで他のノードに伝播させたり、あるいは任意のタイミングで忘却させたいというものがあった。CEC自身の値は、ゲート制御に影響を与えてない。  
覗き穴結合は、CECは自身の値に重み行列を介して伝播可能にした構造となっている。が…実際に期待した効果は得られなかった。  

* [LSTMの実装]()

### GRU
従来のLSTMではパラメータが多数存在し、計算負荷が大きかった。それを解決するため、パタメータを大幅に削減し、精度は同等かそれ以上を期待でいる構造をもつもの。

* 確認テスト  
Q. LSTMとCECが抱える課題について、それぞれ簡潔に述べよ。  
A. LSTMは構造的にパラメータ数が多く、計算負荷がかかる。  
A. CECは、勾配が1で学習能力がない。  
Q. LSTMとGRUのち外を簡潔に述べよ。  
A. LSTMには入力ゲート、出力ゲート、CECの3つの構造があり、パラメータ数が多いため計算負荷が大きいが、GRUにはリセットゲートと更新ゲートの2つの構造で、パラメータ数が少なく計算負荷が小さい。  

* [GRUの実装]()

### 双方向RNN
過去の情報だけでなく、未来の情報を加味することで、精度を向上させるためのモデル

* [双方向RNNの実装]()

## RNNでの自然言語処理
### Seq2Seq  
Encorder-Decoderモデルの1種。時系列データを入力にとって、時系列データを出力する。  

* Endoder RNN  
ユーザーがインプットしたテキストデータを単語等をのトークンんに区切って渡す構造

* Decoder RNN  
システムがアウトプットデータを単語等のトークンごとに生成する構造

* 確認テスト  
Q. 下記の選択肢から、seq2seqについて説明しているものを選べ。  
（1）時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。  
（2）RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。  
（3）構文木などの木構造に対して、隣接単語から表現ベクトル（フレーズ）を作るという演算を再帰的に行い（重みは共通）、文全体の表現ベクトルを得るニューラルネットワークである。  
（4）RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を導入することで解決したものである  
A. (2)

* seq2seqの課題  
文脈が無視され、ただ返答が行われる。

* [Seq2Seqの実装]()

### HRED
Seq2Seqの一問一答しかできない問題を解決するためのもの。過去の発話から次の発話を生成する。前の単語の流れに即して返答することで人間らしい文脈で文章が生成される。

* HREDの課題  
文脈に即し返答ができるが、確率的な多様性に乏しく、同じ発話を与えられると同じような返答しかできない。また、「うん」や「そうだね」などの短い返答をしがちである。

### VHRED
HREDにVAEの潜在変数の概念を追加したもの。

* 確認テスト  
Q. seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。  
A. seq2seqは、1文の一問一答に対して処理ができるある時系列データからある時系列データを作り出すネットワーク。HREDは、それまでの文脈の意味ベクトルを解釈に加えられるようにすることで、文脈の意味を汲み取った文の変換をできるようにしたもの。VHREDは、VAEの考え方を取り入れて短い回答以上の出力ができるようなモデル。

### オートエンコーダー
教師なし学習の一つ。28×28の数字画像を入力とすると、同じ画像を出力するようなモデル

* オートエンコーダーの課題  
潜在変数zにデータを押し込めているので、その構造がどのような状態か分からない。つまり、数値画像1と7のベクトルが似ていると分からないといった感じ。

* VAE  
潜在変数zに確率分布z~N(1,0)を仮定したもの。

* 確認テスト  
Q. VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。自己符号化器の潜在変数に____を導入したもの。  
A. 確率分布  

### Word2vec
RNNの課題である単語のような可変長の文字列をNNに与えることができないという問題を解決する方法。固定長形式で単語を表せるようにした。学習データからボキャブラリを作成し、最終的にembeddingベクトルを作成する。

* [Word2vecの実装]()

### Attension Mechanism
seq2se2の長い文章への対応が難しい問題を解決する方法。2単語でも100単語でも同じベクトル内に表現しなければならないという固定次元ベクトルを文章が長くなるほどに、シーケンスの内部表現の次元も大きくなっていくという仕組みで解消した。

* 確認テスト  
Q. RNNとword2vec、seq2seqとAttentionの違いを簡潔に述べよ。
A. RNNは時系列データを処理するのに適したネットワーク。word2vecは単語の分散表現p￥ベクトルを得る表現。
一つの時系列データから別の時系列データを得るネットワーク。Attensionは時系列データの中身に関して、それぞれの関連性に重みをつける手法。

* [Attension Mechanismの実装]()